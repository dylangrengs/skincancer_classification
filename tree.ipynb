{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Trees Implementation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cv2\n",
    "from collections import Counter\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision Trees Concept</h3>\n",
    "\n",
    "Decision trees are a a set of nodes arranged in a parent-child relationship. The nodes of a decision tree has the following structure:\n",
    "\n",
    "Node {  \n",
    "    self.feature,  \n",
    "    self.threshold,  \n",
    "    self.left,  \n",
    "    self.right,  \n",
    "    self.value,  \n",
    "    self.is_leaf_node  \n",
    "}\n",
    "\n",
    "In order for a decision tree to be a true decision tree, there are various functions used to create the tree:\n",
    "\n",
    "DecisionTree {  \n",
    "    self.fit(),  \n",
    "    self.predict(),  \n",
    "    self._tree_traversal(),  \n",
    "    self._information_gain(),  \n",
    "    self._entropy(),  \n",
    "    self._best_split(),  \n",
    "    self._split()  \n",
    "}\n",
    "\n",
    "For the various functions of a decision tree, there may be additional helper functions to be implemented so as to make the code more organized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Node and Decision Tree Class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "  \n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split = 2, max_depth = 100, n_features = None, split_method = \"gini\"):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.split_method = split_method\n",
    "        self.nodes = 0\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "        self.root = self._grow_tree(X, y)\n",
    "    \n",
    "    '''\n",
    "    _grow_tree(self, X_train, y_train, depth = 0)\n",
    "    Function recursively calls itself until one of the following conditions is met:\n",
    "        1. depth is greater than or equal to the max depth\n",
    "        2. The number of labels left to split are 1\n",
    "        3. The number of samples are less than the minimum number of samples to make a split\n",
    "    Each pass performs each of the following actions (if break condition is not met):\n",
    "        1. Create a list of all possible features that can be split.\n",
    "        2. Retrieve the best threshold and best feature to split using _best_split().\n",
    "        3. Grow the tree by building a node with left and right child nodes (recursion occurs here).\n",
    "    '''\n",
    "    def _grow_tree(self, X, y, depth = 0):\n",
    "        print(\"Depth: {}\".format(depth), end=\"\\r\")\n",
    "        n_samples, n_feats = X.shape # [0], X.shape[3]\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        #stopping criteria\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value = leaf_value)\n",
    "\n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace = False)\n",
    "\n",
    "        #find best split\n",
    "        best_feat, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        #create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feat], best_thresh)\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        self.nodes += 1\n",
    "        return Node(best_feat, best_thresh, left, right)\n",
    "\n",
    "    '''\n",
    "    _best_split(X_train, y_train, feature indicies)\n",
    "    This function computes the best split by calling the information gain for all feature indicies,\n",
    "    which uses entropy. This function returns split_threshold (the threshold at which a value is \"yes\" or \"no\"\n",
    "    at a node) and split_idx (the index where the split occurs in the list of features).\n",
    "    '''\n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        #should be -1 if using information gain\n",
    "        if self.split_method == \"gini\":\n",
    "            best_gain = 1\n",
    "            split_idx, split_threshold = None, None\n",
    "\n",
    "            for feat_idx in feat_idxs:\n",
    "                X_col = X[:, feat_idx]\n",
    "                thresholds = np.unique(X_col)\n",
    "\n",
    "                for thresh in thresholds:\n",
    "                    #gain = self._information_gain(y, X_col, thresh)\n",
    "                    gain = self._gini(y)\n",
    "\n",
    "                    #was gain > best_gain\n",
    "                    if gain < best_gain:\n",
    "                        best_gain = gain\n",
    "                        split_idx = feat_idx\n",
    "                        split_threshold = thresh\n",
    "        else:\n",
    "            best_gain = -1\n",
    "            split_idx, split_threshold = None, None\n",
    "\n",
    "            for feat_idx in feat_idxs:\n",
    "                X_col = X[:, feat_idx]\n",
    "                thresholds = np.unique(X_col)\n",
    "\n",
    "                for thresh in thresholds:\n",
    "                    gain = self._information_gain(y, X_col, thresh)\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        split_idx = feat_idx\n",
    "                        split_threshold = thresh\n",
    "\n",
    "        return split_idx, split_threshold\n",
    "\n",
    "    def _gini(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return 1 - sum(p**2 for p in ps)\n",
    "\n",
    "    '''\n",
    "    _information_gain(y_test, X_col, threshold)\n",
    "    This function computes the information gain (the higher; the better) by computing the entropy\n",
    "    of the parent node and the entropies of the potential children nodes. The information gain is calculated\n",
    "    by the following equation: info_gain = entropy(parent) - (entropy(left_child) + entropy(right_child)).\n",
    "    '''\n",
    "    def _information_gain(self, y, X_col, threshold):\n",
    "        parent_entr = self._entropy(y)\n",
    "        left_idxs, right_idxs = self._split(X_col, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "            return 0\n",
    "\n",
    "        # weighted avg. entropy of children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entr = (n_l/n) * e_l + (n_r/n) * e_r\n",
    "\n",
    "        info_gain = parent_entr - child_entr\n",
    "        return info_gain\n",
    "\n",
    "    def _split(self, X_col, split_thresh):\n",
    "        left_idxs = np.argwhere(X_col <= split_thresh).flatten()\n",
    "        right_idxs = np.argwhere(X_col > split_thresh).flatten()\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log(p) for p in ps if p > 0])\n",
    "            \n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        val = counter.most_common(1)[0][0]\n",
    "        return val\n",
    "    \n",
    "    #predict(X_test)\n",
    "    #returns the value given by the tree after traversing the tree using the rules fit by the training data.\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "    def number_of_nodes(self):\n",
    "        return self.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Setting Up Data</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preparing the Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"..\\data\\HAM10000_metadata.csv\")\n",
    "\n",
    "# Creating a dataframe with 10%\n",
    "# values of original dataframe\n",
    "data = data.sample(frac = 0.08)\n",
    "testData = data.sample(frac = 0.2)\n",
    " \n",
    "# Creating dataframe with\n",
    "# rest of the 84% values\n",
    "trainData = data.drop(testData.index)\n",
    "\n",
    "# class labels\n",
    "classes = [['mel', 'Melanoma'], ['nv', 'Melanocytic nevus'], ['bcc', 'Basal cell carcinoma'], \n",
    "           ['akiec', 'Actinic keratosis / Bowenâ€™s disease'], ['bkl', 'Benign keratosis'], ['df', 'Dermatofibroma'],\n",
    "           ['vasc', 'Vascular lesion']]\n",
    "\n",
    "\n",
    "# row = data.iloc[1]:\n",
    "testImages = []\n",
    "testLabels = []\n",
    "for index, row in testData.iterrows():\n",
    "    if index % 1000 == 0:\n",
    "        print (index)\n",
    "    img = cv2.imread(\"..\\data\\\\allData\\HAM10000_images\\\\\" + row['image_id'] +\".jpg\", flags= cv2.IMREAD_GRAYSCALE)\n",
    "    # img = img / 255  ## changes values 0-1\n",
    "    dim = (108, 81)\n",
    "    img = cv2.resize(img, dim)  ## resize\n",
    "    testImages.append(np.reshape(img, 108*81))\n",
    "    arr = None\n",
    "    if row['dx'] == 'mel':\n",
    "        arr =  0\n",
    "    elif row['dx'] == 'nv':\n",
    "        arr =  1\n",
    "    elif row['dx'] == 'bcc':\n",
    "        arr =  2\n",
    "    elif row['dx'] == 'akiec':\n",
    "        arr =  3\n",
    "    elif row['dx'] == 'bkl':\n",
    "        arr =  4\n",
    "    elif row['dx'] == 'df':\n",
    "        arr =  5\n",
    "    else:\n",
    "        arr =  6\n",
    "    testLabels.append(arr)\n",
    "\n",
    "trainImages = []\n",
    "trainLabels = []\n",
    "for index, row in trainData.iterrows():\n",
    "    if index % 1000 == 0:\n",
    "        print (index)\n",
    "    img = cv2.imread(\"..\\data\\\\allData\\HAM10000_images\\\\\" + row['image_id'] +\".jpg\", flags= cv2.IMREAD_GRAYSCALE)\n",
    "    # img = img / 255  ## changes values 0-1\n",
    "    dim = (108, 81)\n",
    "    img = cv2.resize(img, dim)  ## resize\n",
    "    trainImages.append(np.reshape(img, 108*81))\n",
    "    arr = None\n",
    "    if row['dx'] == 'mel':\n",
    "        arr =  0\n",
    "    elif row['dx'] == 'nv':\n",
    "        arr =  1\n",
    "    elif row['dx'] == 'bcc':\n",
    "        arr =  2\n",
    "    elif row['dx'] == 'akiec':\n",
    "        arr =  3\n",
    "    elif row['dx'] == 'bkl':\n",
    "        arr =  4\n",
    "    elif row['dx'] == 'df':\n",
    "        arr =  5\n",
    "    else:\n",
    "        arr =  6\n",
    "    trainLabels.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = np.array(trainImages)\n",
    "yTrain = np.array(trainLabels)\n",
    "xTest = np.array(testImages)\n",
    "yTest = np.array(testLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fitting the Decision Tree</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6109725685785536\n"
     ]
    }
   ],
   "source": [
    "# clf_tree = DecisionTree()    ## ~62% for both gini and non gini with 100 dpeth 5% of data set train            increasing to 20% data set train got 63% \n",
    "clf_tree = DecisionTree(min_samples_split=2, max_depth= 200)  # 61.09% accuracy\n",
    "clf_tree.fit(xTrain, yTrain)\n",
    "predictions = clf_tree.predict(xTest)\n",
    "\n",
    "def accuracy(y_test, y_pred):\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "acc = accuracy(yTest, predictions)\n",
    "print(\"Accuracy: {}\".format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bde1fcefd68ccd799ca2a32c258060ffb5306650bf50a516f516d3f6435de53b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
